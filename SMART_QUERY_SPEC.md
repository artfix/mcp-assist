# Smart Query Tool - Specification Document

## Overview

A context-aware entity discovery system for Home Assistant that enables LLMs to efficiently find relevant entities without requiring full context dumps or hardcoded intent mappings.

### Problem Statement

| Approach | Pros | Cons |
|----------|------|------|
| Full context dump | LLM has all info, uses own knowledge | ~15k tokens, slow, expensive |
| Targeted queries | Efficient, fast | LLM doesn't know what to search for |
| Hardcoded intents | Fast, accurate | Inflexible, doesn't scale, breaks on different setups |

### Solution

A **dynamic index** generated at startup that tells the LLM what exists in the system, enabling it to make smart, targeted queries using its own domain knowledge.

---

## The Index

### Design Principles

1. **Static only** - No current states (they change)
2. **Structure over values** - What exists, not what it shows
3. **LLM interprets** - No hardcoded thresholds or meanings
4. **Auto-generated** - No manual tagging required
5. **Gap-filling** - LLM infers categories for entities without device_class

### Index Schema

```yaml
# STRUCTURE
areas:
  - name: "Kitchen"
    entity_count: 41
  - name: "Living Room"
    entity_count: 25
  # ... all areas

domains:
  sensor: 180
  binary_sensor: 106
  light: 31
  climate: 18
  media_player: 14
  cover: 12
  # ... all domains with counts

# DEVICE CLASSES (grouped by domain)
device_classes:
  binary_sensor:
    door: 23
    window: 18
    occupancy: 15
    motion: 11
    moisture: 5
    presence: 2
    lock: 2
  sensor:
    temperature: 32
    humidity: 22
    carbon_dioxide: 16
    pressure: 5
    carbon_monoxide: 4
    pm25: 4
    pm10: 4
    pm1: 4
    aqi: 8
    volume_flow_rate: 1
    illuminance: 1
    # ... all device classes
  media_player:
    speaker: 11
    tv: 1
  cover:
    curtain: 5
    blind: 5
    shade: 1

# INFERRED TYPES (for entities WITHOUT device_class)
# Generated by LLM analyzing entity names at index creation
inferred_types:
  person_detection: 6      # *_person_detected
  vehicle_detection: 4     # *_vehicle_detected
  animal_detection: 4      # *_animal_detected
  package_detection: 1     # *_package_detected
  location_tracking: 9     # *_ble_area

# PEOPLE & PETS
people:
  - name: "Michael"
    aliases: ["Mike"]
  - name: "Janette"
    aliases: ["Jeannette", "Jeanette"]
  # ... all people

pets:
  - name: "Lotus"
    type: "cat"
  - name: "Meatball"
    type: "cat"

# CALENDARS
calendars: [Dad, Golf/Gym, Home, Janette, Lana, Michael, Mum, Mya, Security, Work]

# ZONES (for geofencing/presence)
zones: [Home, Churchill Hospital, Didcot Parkway, Frilford Heath Golf Club, ...]

# AUTOMATIONS
automations: [Count Cars on Driveway, Count Pets in Garden, Security Camera Check]

# SCRIPTS
scripts: [script.name1, script.name2, ...]

# INPUT HELPERS (names only)
input_booleans: [Away, Sleep, Enable Motion Lights, Enable Announcements, ...]
input_texts: [Room - Mike, Room - Janette, Room - Lana, ...]
input_numbers: [Cars on Driveway, Cats in Garden, Dogs in Garden]
```

### What's NOT in the Index

| Excluded | Reason |
|----------|--------|
| Current states | Dynamic, changes constantly |
| Units of measurement | LLM already knows what °C, ppm mean |
| Thresholds | LLM has domain knowledge |
| Entity IDs | Too verbose, not needed for discovery |
| Naming patterns | Redundant - covered by device_classes |

### Index Size Target

- Full context dump: ~15,000 tokens
- Index: ~300-400 tokens
- Reduction: 97%

---

## Index Generation

### Startup Flow

```
1. Home Assistant starts
2. mcp-assist loads
3. Generate index:
   a. Query all areas with entity counts
   b. Query all domains with entity counts
   c. Query all device_classes with counts (per domain)
   d. Find entities WITHOUT device_class
   e. Send entity names to LLM for category inference
   f. Compile people, pets, calendars, zones, automations, scripts, input helpers
4. Store index in memory
5. Index available for all queries
```

### LLM Gap-Filling

For entities without device_class, the LLM analyzes names and infers categories:

**Input to LLM:**
```
Entities without device_class:
- binary_sensor.g4_doorbell_pro_person_detected
- binary_sensor.g4_doorbell_pro_vehicle_detected
- binary_sensor.g4_doorbell_pro_animal_detected
- binary_sensor.g4_doorbell_pro_package_detected
- sensor.mike_s_iphone_ble_area
- sensor.janette_iphone_ble_area
...

Categorize these entities into semantic types.
```

**Output from LLM:**
```yaml
inferred_types:
  person_detection:
    pattern: "*_person_detected"
    count: 6
  vehicle_detection:
    pattern: "*_vehicle_detected"
    count: 4
  location_tracking:
    pattern: "*_ble_area"
    count: 9
```

---

## Query Flow

### Step-by-Step

```
1. User asks: "Do we have a leak?"

2. LLM has index in context, reads:
   - device_classes.binary_sensor.moisture: 5
   - device_classes.sensor.volume_flow_rate: 1

3. LLM uses domain knowledge:
   "Leak detection = moisture sensors + water flow meters"

4. LLM calls tools:
   discover_entities(device_class="moisture")
   discover_entities(device_class="volume_flow_rate")

5. Tool returns entity states:
   - Kitchen Sink Leak Sensor: unavailable
   - Ensuite Sink Leak Sensor: off
   - Sonic Volume flow rate: 0.14 L/min
   - ...

6. LLM interprets and responds:
   "No active leaks detected. Water flow is 0.14 L/min.
    Note: Kitchen leak sensor is offline."
```

### Query Examples

| Query | LLM reads from index | LLM queries |
|-------|---------------------|-------------|
| "Do we have a leak?" | moisture: 5, volume_flow_rate: 1 | device_class=moisture, device_class=volume_flow_rate |
| "Is someone at the door?" | occupancy: 15, person_detection: 6, motion: 11 | area=Front Garden, device_class=occupancy |
| "How's the air quality?" | carbon_dioxide: 16, pm25: 4, aqi: 8 | device_class=[carbon_dioxide, pm25, aqi] |
| "Who is home?" | people: [Mike, Janette, ...] | domain=person |
| "Where is Mike?" | location_tracking: 9 | name_contains="mike", name_contains="ble_area" |
| "What lights are on?" | light: 31 | domain=light, state=on |
| "Play some music" | speaker: 11 | device_class=speaker, area={source_area} |

---

## Tool Enhancements Required

### Current discover_entities

```python
discover_entities(
    area: str,
    domain: str,
    entity_type: str,
    name_contains: str,
    state: str,
    limit: int
)
```

### Required Additions

```python
discover_entities(
    # Existing
    area: str,
    domain: str,
    entity_type: str,
    name_contains: str,
    state: str,
    limit: int,

    # NEW
    device_class: str | list[str],     # Filter by device_class
    name_pattern: str,                  # Wildcard support: "*_person_detected"
)
```

### New Tool: get_index

```python
get_index() -> Index
# Returns the pre-generated index
# Called once at start of conversation
```

---

## Real-World Examples

### Example 1: "Who is home and where are they?"

**Level 0-2 (Without Index):**
- Queries person domain, gets who is home
- Doesn't know to query BLE area sensors for room location
- Misses the full answer

**Level 3 (With Index):**
```
LLM reads index:
- people: [Mike, Janette, Lana, Mya, Brian, Patricia]
- location_tracking: 9 (inferred from *_ble_area)

LLM queries:
- discover_entities(domain="person", state="home")
- discover_entities(name_contains="ble_area")

Result:
- Mike: home, Studio
- Janette: home, Living Room
- Lana: home, Lana's Bedroom
- Mya: home, unknown
```

### Example 2: "Do we have a leak?"

**Without Index:**
- Might search for "leak" in entity names
- Misses water flow sensor (no "leak" in name)
- Misses soil moisture (would incorrectly include it)

**With Index:**
```
LLM reads index:
- device_classes.binary_sensor.moisture: 5
- device_classes.sensor.volume_flow_rate: 1
- device_classes.sensor.moisture: 1 (soil - LLM knows to exclude)

LLM queries:
- discover_entities(device_class="moisture", domain="binary_sensor")
- discover_entities(device_class="volume_flow_rate")

Result:
- 5 leak sensors (2 off, 3 unavailable/unknown)
- Flow rate: 0.14 L/min
- Excludes soil moisture (outdoor, not leak detection)
```

### Example 3: "Is someone at the door?"

**Without Index:**
- Searches "door" - gets 23 door contact sensors
- Answers about door open/closed, not person presence

**With Index:**
```
LLM reads index:
- person_detection: 6 (inferred type)
- device_classes.binary_sensor.occupancy: 15
- areas: Front Garden (9 entities)

LLM understands:
- "The door" = front door = Front Garden area
- "Someone" = person detection, not door state

LLM queries:
- discover_entities(area="Front Garden", device_class="occupancy")
- discover_entities(name_contains="person_detected", area="Front Garden")

Result:
- G4 Doorbell Pro Person detected: off
- Front Porch Presence Sensor: off
- "No, nobody is at the front door"
```

### Example 4: "How's the air quality?"

**Without Index:**
- Doesn't know what sensors measure air quality
- Might miss CO2, VOC, PM2.5 sensors

**With Index:**
```
LLM reads index:
- carbon_dioxide: 16
- carbon_monoxide: 4
- pm25: 4
- pm10: 4
- aqi: 8

LLM queries:
- discover_entities(device_class=["carbon_dioxide", "pm25", "aqi"])

Result grouped by room:
- Bedroom: CO2 689ppm (good), PM2.5 0.5µg/m³ (excellent)
- Kitchen: CO2 433ppm (excellent)
- Lana's Bedroom: CO2 1113ppm (elevated - needs ventilation)
```

### Example 5: "Play some music" (from Studio)

**Without Index:**
- Doesn't know source room
- Lists all 14 media players

**With Index:**
```
LLM reads index:
- device_classes.media_player.speaker: 11
- areas: Studio (31 entities)

LLM knows:
- Request source: Studio
- Need speaker in Studio

LLM queries:
- discover_entities(area="Studio", device_class="speaker")

Result:
- Sonos Studio: paused
- Action: Resume or play new music on Studio speaker
```

---

## Integration Points

### For Voice Assist Pipeline

1. Index loaded into conversation context at session start
2. LLM uses index to understand available entities
3. LLM makes targeted discover_entities calls
4. Faster response times due to smaller queries

### For MCP Clients (Claude Code, etc.)

1. `get_index()` tool returns full index
2. Client LLM stores in context
3. Subsequent queries use index for smart discovery

---

## Performance Comparison

| Approach | Tokens | Round-trips | Latency |
|----------|--------|-------------|---------|
| Full dump every query | ~15,000 | 1 | High (processing) |
| Blind targeted queries | ~500 | 1-3 | Medium (might miss entities) |
| Index + targeted queries | ~400 + ~500 | 2 | Low (accurate, efficient) |

---

## Implementation Phases

### Phase 1: Core Index
- [ ] Generate index at startup
- [ ] Include: areas, domains, device_classes
- [ ] Store in memory

### Phase 2: Gap Filling
- [ ] Identify entities without device_class
- [ ] LLM inference for inferred_types
- [ ] Include: people, pets, calendars, zones

### Phase 3: Tool Enhancements
- [ ] Add device_class filter to discover_entities
- [ ] Add name_pattern wildcard support
- [ ] Create get_index tool

### Phase 4: Integration
- [ ] Voice Assist pipeline integration
- [ ] MCP tool exposure
- [ ] Testing with real-world queries

---

## Open Questions

1. **Index refresh** - When should index be regenerated? On entity registry changes? Periodically?

2. **Index storage** - Memory only, or persist to disk for faster startup?

3. **LLM for gap-filling** - Which LLM? Local? Cloud? Cost considerations?

4. **Partial index** - Should there be a "lite" version for constrained contexts?
